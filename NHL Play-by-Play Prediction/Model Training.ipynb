{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the basic paths\n",
    "data_path = '../../nhl-game-data/'\n",
    "preproc_data_path = '../data/'\n",
    "\n",
    "# Read in the data on each game\n",
    "per_game = pd.read_csv(os.path.join(preproc_data_path, 'per-game.csv'))\n",
    "per_game['game_id'] = per_game['GameId']\n",
    "# Read in the data on each event within each game\n",
    "game_plays = pd.read_csv(os.path.join(data_path, 'game_plays.csv'))\n",
    "# Read in the data on summary statistics within each game for both teams\n",
    "game_teams = pd.read_csv(os.path.join(data_path, 'game_teams_stats.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all of the home and away games for each team\n",
    "g_home = game_teams[game_teams['HoA'] == 'home'][['game_id', 'team_id', 'goals']].copy()\n",
    "g_away = game_teams[game_teams['HoA'] == 'away'][['game_id', 'team_id', 'goals']].copy()\n",
    "\n",
    "# Combine them into a wider dataframe\n",
    "game_summary = pd.merge(g_home, g_away, on='game_id', suffixes=('_home', '_away'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want \"action\" events, where there is a doer and receiver team, i.e. not things like timeout\n",
    "action_plays = game_plays[~game_plays['team_id_for'].isna() & ~game_plays['team_id_against'].isna()].copy()\n",
    "\n",
    "# Create a continuous time measure out of the period & periodTime variables\n",
    "action_plays['time'] = action_plays['period'] - 1.0 + \\\n",
    "                       action_plays['periodTime'] / (action_plays['periodTime'] + action_plays['periodTimeRemaining'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the game-level covariates to the event dataframe created above\n",
    "events = pd.merge(action_plays, game_summary[['game_id', 'team_id_home', 'team_id_away']] , on='game_id')\n",
    "# Append a 1 (Home) and -1 (Away) indicator for which side initated the action\n",
    "events['h_a_sign'] = (events['team_id_for'].astype(int) == events['team_id_home']).astype(float) * 2.0 - 1.0\n",
    "\n",
    "# Remove unused variables from the data, such as play ids, old time measures, and player locations\n",
    "# NOTE: In future iterations we can refine the model to also take into account player locations\n",
    "remove_cols = ['play_id', 'play_num', 'team_id_for', 'team_id_against', 'period', \n",
    "               'periodTime', 'periodTimeRemaining', 'dateTime', \n",
    "               'description', 'rink_side', 'secondaryType', 'x', 'y', 'st_x', 'st_y',\n",
    "               'team_id_home', 'team_id_away']\n",
    "events.drop(remove_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the game information with summary measures of the relevant event types\n",
    "tmp = pd.merge(per_game, game_summary[['game_id', 'team_id_home', 'team_id_away']], on='game_id')\n",
    "# Multiply each summary measure by 1 (if Home) or -1 (if Away)\n",
    "tmp.iloc[:, 2:-3] *= (tmp['TeamId'] == tmp['team_id_home']).map({True: 1.0, False: -1.0})[:, None]\n",
    "# Drop the duplicate columns\n",
    "tmp.drop(['TeamId', 'GameId', 'team_id_home', 'team_id_away'], axis=1, inplace=True)\n",
    "\n",
    "# Aggregate all of the relevant measures by game and sum them up to create a history for each of the measures\n",
    "# NOTE: Since Home == 1, Away == -1, we are calculating the difference between home & away performance for each game\n",
    "game_history = tmp.groupby('game_id').sum()\n",
    "# Normalize the history measure\n",
    "game_history = (game_history - game_history.mean()) / (game_history.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all of the game ids for which we have event data for\n",
    "all_game_ids = sorted(events['game_id'].unique())\n",
    "# Drop all games for which we DO NOT have event data for\n",
    "game_history = game_history[game_history.index.isin(all_game_ids)].copy()\n",
    "game_summary = game_summary[game_summary['game_id'].isin(all_game_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally, set game_id as the index for the data\n",
    "events.set_index('game_id', inplace=True)\n",
    "game_summary.set_index('game_id', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of event types; We add 1 for the <GAME_START> event.\n",
    "NUM_EVENT_TYPES = 1 + events['event'].unique().size\n",
    "# Create a mapping from the event type to its unique id\n",
    "EVENT_TYPE_DICT = {e_type: i + 1 for i, e_type in enumerate(events['event'].unique())}\n",
    "# Count the number of history measures we have\n",
    "HIST_DIM = game_history.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEventsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, game_ids, game_history, game_summary, events):\n",
    "        self.game_ids = game_ids\n",
    "        self.game_history = game_history.copy()\n",
    "        self.game_summary = game_summary.copy()\n",
    "        self.events = events.copy()\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        # Select the appropriate game\n",
    "        game_id = self.game_ids[index]\n",
    "        \n",
    "        # Game statistics for the given game\n",
    "        s = self.game_summary.loc[game_id]\n",
    "        team_h = s['team_id_home']\n",
    "        team_a = s['team_id_away']\n",
    "        g_h = s['goals_home']\n",
    "        g_a = s['goals_away']\n",
    "\n",
    "        # NOTE: Score is our outcome of interest\n",
    "        score = torch.tensor(float(g_h - g_a))\n",
    "        # But we also care about which team is winning: 0 (Home is losing), 1 (Tie), 2 (Home is winning)\n",
    "        target = (torch.sign(score) + 1.0).long()\n",
    "        \n",
    "        # Game history tensor\n",
    "        h = torch.Tensor(self.game_history.loc[game_id])\n",
    "\n",
    "        # Subset all of the events to the game of interest\n",
    "        e = self.events.loc[game_id]\n",
    "        # Create an event dataframe with dimensions: (# of events in game) x (# of event types)\n",
    "        e_X = np.zeros((e.shape[0], NUM_EVENT_TYPES))\n",
    "        # One-hot encode the events in the given game\n",
    "        e_X[np.arange(e_X.shape[0]), e['event'].map(EVENT_TYPE_DICT)] = 1.0\n",
    "        # Multiply the one-hot encoding by the 1, -1 Home/Away indicator\n",
    "        e_X *= e['h_a_sign'][:, None]\n",
    "        # And finally, create the event tensor\n",
    "        X = np.array(e_X)\n",
    "        X = torch.Tensor(X)\n",
    "\n",
    "        # Also create a tensor out of the continuous time measure from before\n",
    "        t = np.array(e['time'])\n",
    "        t = torch.Tensor(t)\n",
    "        \n",
    "        # In addition to the overall score measure, we also want one at each time point\n",
    "        s_t = torch.Tensor(np.array(e['goals_home'] - e['goals_away']))\n",
    "        \n",
    "        # For each game, return the history for both teams, time, events,\n",
    "        #   game result, final score, score at each of the observed time points\n",
    "        return h, t, X, target, score, s_t\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.game_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the data into training and test games\n",
    "N = len(all_game_ids)\n",
    "# First 80% of all games are used as training data, rest is test\n",
    "train_game_ids = all_game_ids[:int(0.8 * N)]\n",
    "test_game_ids = all_game_ids[int(0.8 * N):]\n",
    "\n",
    "# And create the PyTorch training and test dataset from the above ids\n",
    "train_ds = GameEventsDataset(train_game_ids, game_history, game_summary, events)\n",
    "test_ds = GameEventsDataset(test_game_ids, game_history, game_summary, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PyTorch training and test data loader from the above training and test data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=5,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=5,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition of MD-RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture Density Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDLayer(nn.Module):\n",
    "    def __init__(self, input_size, K=5):\n",
    "        super(MDLayer, self).__init__()\n",
    "        \n",
    "        # Mixture probabilities for K Normals\n",
    "        self.linear_pi = nn.Linear(input_size, K)\n",
    "        # Means and variances of Normal densities\n",
    "        self.linear_mu = nn.Linear(input_size, K)\n",
    "        self.linear_var = nn.Linear(input_size, K)\n",
    "        \n",
    "        # Initialize the intercept for the means to be evenly spaced between -5 and 5\n",
    "        self.linear_mu.bias.data.copy_(torch.linspace(-5, 5, K, dtype=torch.float32))\n",
    "        # Initialize the intercept for the variances to be -0.5 for all mixtures.\n",
    "        self.linear_var.bias.data.fill_(-0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits_pi = self.linear_pi(x)\n",
    "        mu = self.linear_mu(x)\n",
    "        # NOTE: Working with log-variance, so keep it between ~0.1 and ~7.4\n",
    "        log_var = torch.clamp(self.linear_var(x), min=-2.0, max=2.0)\n",
    "        return logits_pi, mu, log_var"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture Density RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDRNN(nn.Module):\n",
    "    def __init__(self, h_in_dim, e_in_dim, hidden_size=100, K=5):\n",
    "        super(MDRNN, self).__init__()\n",
    "        # History transformation layers\n",
    "        self.history_transform = nn.Sequential(\n",
    "            nn.Linear(h_in_dim, 100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100, hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Mixture density layers\n",
    "        self.mdn_net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.Tanh(),\n",
    "            MDLayer(input_size=50, K=K)\n",
    "        )\n",
    "        \n",
    "        # Layers to transform further to get them ready for auxiliary output calculation\n",
    "        self.aux_z = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, 50),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        # Auxiliary output 1: Game outcome (Home Loss, Tie, Home Win)\n",
    "        self.aux_o = nn.Linear(50, 3)\n",
    "        \n",
    "        # Auxiliary output 2: Current score (Continuous)\n",
    "        self.aux_s_t = nn.Linear(50, 1)\n",
    "        \n",
    "        # Recurrent layer\n",
    "        self.gru = nn.GRU(input_size=e_in_dim,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True)\n",
    "        \n",
    "    def apply_aux(self, x):\n",
    "        # Apply the initial auxiliary transformation\n",
    "        z = self.aux_z(x)\n",
    "        # Predict the outcome probabilities of the game\n",
    "        o_logits = self.aux_o(z)\n",
    "        # And the current score\n",
    "        s_t = self.aux_s_t(z).squeeze(-1)\n",
    "        # Return both the outcome probabilities and score\n",
    "        return o_logits, s_t\n",
    "    \n",
    "    def process_history(self, h):\n",
    "        # Transform the history\n",
    "        h_features = self.history_transform(h)\n",
    "        # Estimate the mixture density parameters\n",
    "        h_out = self.mdn_net(h_features)\n",
    "        # And the auxiliary outputs for the history\n",
    "        h_aux = self.apply_aux(h_features)\n",
    "        # Return all three of those\n",
    "        return h_out, h_features, h_aux\n",
    "        \n",
    "    def forward(self, h, t, X):\n",
    "        # First step is to process the history up until this game\n",
    "        h_out, h_features, h_aux = self.process_history(h)\n",
    "        \n",
    "        # The recurrent input is the one-hot encoded event types\n",
    "        gru_in = X\n",
    "        # The initial state are the transformed history features\n",
    "        gru_state_0 = h_features.unsqueeze(1)\n",
    "        \n",
    "        # Pass the data through the GRU layer\n",
    "        gru_out, gru_state_n = self.gru(gru_in, gru_state_0)\n",
    "        \n",
    "        # Estimate the mixture density parameters\n",
    "        e_out = self.mdn_net(gru_out)\n",
    "        # And the auxiliary outputs\n",
    "        e_aux = self.apply_aux(gru_out)\n",
    "\n",
    "        # Return the initial and post-GRU mixture density parameters, and auxiliary outputs\n",
    "        return h_out, e_out, h_aux, e_aux\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate the log-likelihood of a Normal\n",
    "def normal_log_pdf(mu, log_var, score):\n",
    "    log_C = -0.5 * (np.log(2.0 * np.pi) + log_var)\n",
    "    \n",
    "    shape = [score.size(0)] + [1] * max(mu.ndim - 2, 0) + [1]\n",
    "    score_view = score.view(*shape)\n",
    "    return log_C - 0.5 * ((mu - score_view) ** 2 / torch.exp(log_var))\n",
    "\n",
    "# Helper function to calculate the log-likelihood of the full mixture density model\n",
    "def mdn_log_likelihood(out, score):\n",
    "    logits_pi, mu, log_var = out\n",
    "    log_normal = normal_log_pdf(mu, log_var, score)\n",
    "    \n",
    "    log_pi_norm = torch.logsumexp(logits_pi, dim=-1, keepdim=True)\n",
    "    log_pi = logits_pi - log_pi_norm\n",
    "    \n",
    "    log_v = log_pi + log_normal\n",
    "    log_ll = torch.logsumexp(log_v, dim=-1)\n",
    "    return log_ll\n",
    "\n",
    "# Calculate the cross entropy loss for the game outcomes for the history\n",
    "def compute_h_aux_loss(h_aux, target):\n",
    "    o_logits, s_t_pred = h_aux\n",
    "    return nn.functional.cross_entropy(o_logits, target)\n",
    "\n",
    "def compute_e_aux_loss(e_aux, target, s_t, score_weight=1.0):\n",
    "    # Calculate the cross entropy loss for the game outcomes for the events\n",
    "    def event_loss(out, target):\n",
    "        p = out.view(-1, 3)\n",
    "        return nn.functional.cross_entropy(p, target.repeat(p.size(0)))\n",
    "    o_logits, s_t_pred = e_aux\n",
    "    # Cross-Entropy loss for the outcome probabilities\n",
    "    t_loss = event_loss(o_logits, target)\n",
    "    # MSE loss for the current score difference\n",
    "    s_loss = nn.functional.mse_loss(s_t_pred, s_t)\n",
    "    return t_loss + s_loss * score_weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MD-RNN Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The custom loss function for our network\n",
    "def get_mdn_loss_fn(pretraining=False, history_weight=2.0, score_weight=1.0):\n",
    "    def loss_fn(model, data):\n",
    "        # Parse out the inputs from the data\n",
    "        h, t, X, target, score, s_t = data\n",
    "        # Get the outputs from the model\n",
    "        h_out, e_out, h_aux, e_aux = model(h, t, X)\n",
    "\n",
    "        # Want the negative log-likelihoods for the mixture densities\n",
    "        h_nll = -1.0 * mdn_log_likelihood(h_out, score)\n",
    "        e_nll = -1.0 * mdn_log_likelihood(e_out, score)\n",
    "        \n",
    "        # Calculate the losses for the auxiliary outputs\n",
    "        h_aux_loss = compute_h_aux_loss(h_aux, target)\n",
    "        e_aux_loss = compute_e_aux_loss(e_aux, target, s_t, score_weight=score_weight)\n",
    "        \n",
    "        # Don't include the MD negative log-likelihood in the pre-training\n",
    "        nll_weight = 0.0 if pretraining else 1.0\n",
    "\n",
    "        # Calculate the total losses for both the pre-GRU and post-GRU parameters\n",
    "        e_total = e_aux_loss + nll_weight * torch.mean(e_nll)\n",
    "        h_total = h_aux_loss + nll_weight * h_nll\n",
    "\n",
    "        # And return a weighted sum of them\n",
    "        return e_total + history_weight * h_total\n",
    "    return loss_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MD-RNN Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to see whether the prediction at time t is correct\n",
    "def correct_at_t(t, logits_series, target, cut_off=1.0):\n",
    "    # Find the last event index before the cut_off\n",
    "    ind = torch.max(t * (t < cut_off).float(), dim=-1)[1]\n",
    "    # Gather all of the logits at the event right before the cut_off\n",
    "    logits_at_t = logits_series[np.arange(t.size(0)), ind, :]\n",
    "    # Get the outcome prediction\n",
    "    p = torch.max(logits_at_t, dim=-1)[1]\n",
    "    # And check whether it was correct\n",
    "    return torch.eq(p, target).sum()\n",
    "\n",
    "def evaluate_mdrnn(model, loader):\n",
    "    # Tell the model that we are evauluating\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the negative log-likelihoods\n",
    "    h_nll_sum = 0.0\n",
    "    e_nll_sum = 0.0\n",
    "    \n",
    "    # Initialize the auxiliary losses\n",
    "    h_aux_loss_sum = 0.0\n",
    "    e_aux_loss_sum = 0.0\n",
    "\n",
    "    # Initialize the number of correct predictions outcome predictions\n",
    "    h_correct = 0.0\n",
    "    e_correct_1 = 0.0\n",
    "    e_correct_2 = 0.0\n",
    "    # And the MSE for the current score\n",
    "    e_mse_sum = 0.0\n",
    "\n",
    "    # DO NOT attach the gradients\n",
    "    with torch.no_grad():\n",
    "        # For each observation in the data\n",
    "        for data in loader:\n",
    "            # Parse out the input and outputs\n",
    "            data = tuple(tensor.cuda() for tensor in data)\n",
    "            h, t, X, target, score, s_t = data\n",
    "            \n",
    "            # Calculate the negative log-likelihood for the history MD model\n",
    "            h_out, e_out, h_aux, e_aux = model(h, t, X)\n",
    "            h_nll = -1.0 * mdn_log_likelihood(h_out, score)\n",
    "            h_nll_sum += h_nll.item() * h.size(0)\n",
    "            \n",
    "            # Calculate the negative log-likelihood for the event MD model\n",
    "            e_nll = -1.0 * mdn_log_likelihood(e_out, score)\n",
    "            e_nll_sum += torch.mean(e_nll).item() * h.size(0)\n",
    "            \n",
    "            # Calculate the auxiliary loss for the history outcome model\n",
    "            h_aux_loss = compute_h_aux_loss(h_aux, target)\n",
    "            h_aux_loss_sum += h_aux_loss.item() * h.size(0)\n",
    "            \n",
    "            # Calculate the auxiliary loss for the event outcome model\n",
    "            e_aux_loss = compute_e_aux_loss(e_aux, target, s_t)\n",
    "            e_aux_loss_sum += e_aux_loss.item() * h.size(0)\n",
    "            \n",
    "            # Extract the predicted game outcome for the history model\n",
    "            h_label = torch.max(h_aux[0], dim=-1)[1]\n",
    "            # Add 1 if it got the outcome correct\n",
    "            h_correct += torch.eq(h_label, target).sum()\n",
    "            \n",
    "            # Check whether the outcome for before the first period is correct\n",
    "            e_correct_1 += correct_at_t(t, e_aux[0], target, cut_off=1.0)\n",
    "            # Check whether the outcome for before the second period is correct\n",
    "            e_correct_2 += correct_at_t(t, e_aux[0], target, cut_off=2.0)\n",
    "            # Calculate the MSE loss for the current score difference\n",
    "            e_mse_sum += nn.functional.mse_loss(e_aux[1], s_t).item() * h.size(0)\n",
    "    \n",
    "    # Tell the model that we are no longer evaluating, but training again\n",
    "    model.train()\n",
    "    N = len(loader.dataset)\n",
    "    # Return a nice summary of all of the measures above\n",
    "    return {\n",
    "        'h_nll': h_nll_sum / N,\n",
    "        'e_nll': e_nll_sum / N,\n",
    "        'h_aux_loss': h_aux_loss_sum / N,\n",
    "        'e_aux_loss': e_aux_loss_sum / N,\n",
    "        'acc_0': h_correct / N * 100.0,\n",
    "        'acc_1': e_correct_1 / N * 100.0,\n",
    "        'acc_2': e_correct_2 / N * 100.0,\n",
    "        'e_rmse': np.sqrt(e_mse_sum / N),\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Training Loop Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "def traininig_loop(model, optimizer, train_loader, test_loader, loss_fn, eval_fn, num_steps=10000, eval_interval=1000):\n",
    "    # Inform the model, that we are training it and not evaluating\n",
    "    model.train()\n",
    "    loader_it = iter(train_loader)\n",
    "\n",
    "    K = 0\n",
    "    loss_queue = []\n",
    "    t_0 = time.time()\n",
    "    # For each iteration\n",
    "    for step in range(num_steps):\n",
    "        # Continuously load in the data\n",
    "        try:\n",
    "            data = next(loader_it)\n",
    "        # And restart once we run out\n",
    "        except StopIteration:\n",
    "            loader_it = iter(train_loader)\n",
    "            data = next(loader_it)\n",
    "        \n",
    "        # h, t, X, target, score, s_t\n",
    "        data = tuple(tensor.cuda() for tensor in data)\n",
    "        \n",
    "        # Calculate the loss of the model with the given data\n",
    "        loss = loss_fn(model, data)\n",
    "\n",
    "        # Zero-out your gradient before updating the parameters\n",
    "        optimizer.zero_grad()\n",
    "        # Back propagate to store the new gradients for all parameters\n",
    "        loss.backward()\n",
    "        # Do gradient-descent based on the new gradients to update parameter values\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get loss from given iteration\n",
    "        loss_v = loss.item()\n",
    "        # Increment the time-averaging step counter\n",
    "        K += 1\n",
    "        # If we reached the evaluation interval length, pop the oldest losses\n",
    "        if len(loss_queue) == eval_interval:\n",
    "            loss_queue.pop(0)\n",
    "        # Keep appending the loss to the queue\n",
    "        loss_queue.append(loss_v)\n",
    "        \n",
    "        # Print out training stats at the given eval_interval speed\n",
    "        if step % eval_interval == 0 or step == num_steps - 1:\n",
    "            # Evaluate the model\n",
    "            eval_res = eval_fn(model, test_loader)\n",
    "            # And create the measures of interest as a string\n",
    "            eval_res_strings = ['%s: %.2f' % (name, val) for name, val in eval_res.items()]\n",
    "            eval_str = ' '.join(eval_res_strings)\n",
    "            \n",
    "            # Calculate the time it took to take K steps\n",
    "            dt = (time.time() - t_0) / K\n",
    "\n",
    "            # Print out the step count, mean of the losses so far, time/iteration, and the evaluation results\n",
    "            print('Step: %05d. Loss: %.4f Time/it: %.2f sec.\\n%s' % (step, np.mean(loss_queue), dt, eval_str))\n",
    "            # Print a new line\n",
    "            print()\n",
    "\n",
    "            # Start measuring time and steps from scratch again\n",
    "            t_0 = time.time()\n",
    "            K = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the MD-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDRNN(\n",
       "  (history_transform): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (5): Tanh()\n",
       "  )\n",
       "  (mdn_net): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): MDLayer(\n",
       "      (linear_pi): Linear(in_features=50, out_features=7, bias=True)\n",
       "      (linear_mu): Linear(in_features=50, out_features=7, bias=True)\n",
       "      (linear_var): Linear(in_features=50, out_features=7, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (aux_z): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       "  (aux_o): Linear(in_features=50, out_features=3, bias=True)\n",
       "  (aux_s_t): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (gru): GRU(11, 100, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the mixture density RNN with 7 Normals\n",
    "mdrnn = MDRNN(h_in_dim=HIST_DIM, e_in_dim=NUM_EVENT_TYPES, K=7)\n",
    "mdrnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 00000. Loss: 20.1904 Time/it: 21.17 sec.\n",
      "h_nll: 2.49 e_nll: 2.49 h_aux_loss: 1.14 e_aux_loss: 3.74 acc_0: 6.63 acc_1: 6.54 acc_2: 6.54 e_rmse: 1.61\n",
      "\n",
      "Step: 01000. Loss: 11.9844 Time/it: 0.04 sec.\n",
      "h_nll: 2.34 e_nll: 2.24 h_aux_loss: 0.89 e_aux_loss: 2.33 acc_0: 42.73 acc_1: 61.09 acc_2: 67.27 e_rmse: 1.23\n",
      "\n",
      "Step: 02000. Loss: 10.8938 Time/it: 0.04 sec.\n",
      "h_nll: 2.30 e_nll: 2.23 h_aux_loss: 0.91 e_aux_loss: 2.67 acc_0: 50.78 acc_1: 60.16 acc_2: 61.27 e_rmse: 1.34\n",
      "\n",
      "Step: 03000. Loss: 9.8888 Time/it: 0.04 sec.\n",
      "h_nll: 2.27 e_nll: 2.02 h_aux_loss: 0.89 e_aux_loss: 0.99 acc_0: 50.96 acc_1: 64.12 acc_2: 74.79 e_rmse: 0.53\n",
      "\n",
      "Step: 04000. Loss: 9.4214 Time/it: 0.04 sec.\n",
      "h_nll: 2.30 e_nll: 2.16 h_aux_loss: 0.90 e_aux_loss: 1.86 acc_0: 49.62 acc_1: 62.61 acc_2: 71.10 e_rmse: 1.04\n",
      "\n",
      "Step: 05000. Loss: 9.3849 Time/it: 0.04 sec.\n",
      "h_nll: 2.29 e_nll: 1.98 h_aux_loss: 0.91 e_aux_loss: 0.77 acc_0: 49.58 acc_1: 64.34 acc_2: 75.19 e_rmse: 0.25\n",
      "\n",
      "Step: 06000. Loss: 9.2065 Time/it: 0.04 sec.\n",
      "h_nll: 2.28 e_nll: 1.97 h_aux_loss: 0.91 e_aux_loss: 0.88 acc_0: 49.98 acc_1: 64.87 acc_2: 75.32 e_rmse: 0.42\n",
      "\n",
      "Step: 07000. Loss: 9.1789 Time/it: 0.04 sec.\n",
      "h_nll: 2.28 e_nll: 2.01 h_aux_loss: 0.92 e_aux_loss: 0.85 acc_0: 50.78 acc_1: 64.87 acc_2: 75.72 e_rmse: 0.36\n",
      "\n",
      "Step: 08000. Loss: 9.2630 Time/it: 0.04 sec.\n",
      "h_nll: 2.27 e_nll: 1.98 h_aux_loss: 0.89 e_aux_loss: 0.91 acc_0: 51.18 acc_1: 65.18 acc_2: 74.74 e_rmse: 0.45\n",
      "\n",
      "Step: 09000. Loss: 9.1357 Time/it: 0.04 sec.\n",
      "h_nll: 2.27 e_nll: 1.93 h_aux_loss: 0.89 e_aux_loss: 0.72 acc_0: 50.87 acc_1: 65.18 acc_2: 75.72 e_rmse: 0.19\n",
      "\n",
      "Step: 10000. Loss: 9.0810 Time/it: 0.04 sec.\n",
      "h_nll: 2.28 e_nll: 1.95 h_aux_loss: 0.91 e_aux_loss: 0.76 acc_0: 50.78 acc_1: 65.05 acc_2: 75.59 e_rmse: 0.27\n",
      "\n",
      "Step: 11000. Loss: 9.0738 Time/it: 0.04 sec.\n",
      "h_nll: 2.30 e_nll: 1.97 h_aux_loss: 0.91 e_aux_loss: 0.92 acc_0: 50.78 acc_1: 64.52 acc_2: 75.28 e_rmse: 0.46\n",
      "\n",
      "Step: 12000. Loss: 9.0742 Time/it: 0.04 sec.\n",
      "h_nll: 2.27 e_nll: 1.92 h_aux_loss: 0.91 e_aux_loss: 0.77 acc_0: 51.13 acc_1: 65.14 acc_2: 76.35 e_rmse: 0.27\n",
      "\n",
      "Step: 13000. Loss: 9.1017 Time/it: 0.04 sec.\n",
      "h_nll: 2.30 e_nll: 2.01 h_aux_loss: 0.92 e_aux_loss: 0.88 acc_0: 50.78 acc_1: 65.27 acc_2: 75.68 e_rmse: 0.39\n",
      "\n",
      "Step: 14000. Loss: 9.1754 Time/it: 0.04 sec.\n",
      "h_nll: 2.28 e_nll: 1.93 h_aux_loss: 0.92 e_aux_loss: 0.79 acc_0: 48.69 acc_1: 65.50 acc_2: 74.74 e_rmse: 0.29\n",
      "\n",
      "Step: 15000. Loss: 9.0397 Time/it: 0.04 sec.\n",
      "h_nll: 2.26 e_nll: 1.91 h_aux_loss: 0.89 e_aux_loss: 0.73 acc_0: 50.64 acc_1: 65.72 acc_2: 76.35 e_rmse: 0.22\n",
      "\n",
      "Step: 16000. Loss: 9.0231 Time/it: 0.04 sec.\n",
      "h_nll: 2.28 e_nll: 1.91 h_aux_loss: 0.89 e_aux_loss: 0.77 acc_0: 50.07 acc_1: 65.54 acc_2: 75.54 e_rmse: 0.28\n",
      "\n",
      "Step: 17000. Loss: 9.0658 Time/it: 0.04 sec.\n",
      "h_nll: 2.27 e_nll: 1.90 h_aux_loss: 0.90 e_aux_loss: 0.83 acc_0: 50.78 acc_1: 64.78 acc_2: 75.28 e_rmse: 0.37\n",
      "\n",
      "Step: 18000. Loss: 9.1242 Time/it: 0.04 sec.\n",
      "h_nll: 2.26 e_nll: 1.95 h_aux_loss: 0.90 e_aux_loss: 0.77 acc_0: 48.51 acc_1: 66.47 acc_2: 76.66 e_rmse: 0.27\n",
      "\n",
      "Step: 19000. Loss: 8.9918 Time/it: 0.04 sec.\n",
      "h_nll: 2.26 e_nll: 1.89 h_aux_loss: 0.89 e_aux_loss: 0.72 acc_0: 49.89 acc_1: 66.03 acc_2: 76.83 e_rmse: 0.19\n",
      "\n",
      "Step: 20000. Loss: 8.8542 Time/it: 0.04 sec.\n",
      "h_nll: 2.28 e_nll: 1.92 h_aux_loss: 0.90 e_aux_loss: 0.71 acc_0: 50.78 acc_1: 66.47 acc_2: 77.23 e_rmse: 0.16\n",
      "\n",
      "Step: 21000. Loss: 9.0745 Time/it: 0.04 sec.\n",
      "h_nll: 2.27 e_nll: 1.88 h_aux_loss: 0.90 e_aux_loss: 0.74 acc_0: 50.11 acc_1: 66.47 acc_2: 76.35 e_rmse: 0.22\n",
      "\n",
      "Step: 22000. Loss: 8.9041 Time/it: 0.04 sec.\n",
      "h_nll: 2.27 e_nll: 1.89 h_aux_loss: 0.90 e_aux_loss: 0.70 acc_0: 50.42 acc_1: 66.74 acc_2: 77.32 e_rmse: 0.15\n",
      "\n",
      "Step: 23000. Loss: 9.0177 Time/it: 0.04 sec.\n",
      "h_nll: 2.27 e_nll: 1.91 h_aux_loss: 0.91 e_aux_loss: 0.71 acc_0: 50.78 acc_1: 66.79 acc_2: 77.63 e_rmse: 0.15\n",
      "\n",
      "Step: 24000. Loss: 9.0963 Time/it: 0.04 sec.\n",
      "h_nll: 2.28 e_nll: 1.94 h_aux_loss: 0.91 e_aux_loss: 0.74 acc_0: 50.73 acc_1: 67.41 acc_2: 78.17 e_rmse: 0.20\n",
      "\n",
      "Step: 24999. Loss: 9.0657 Time/it: 0.04 sec.\n",
      "h_nll: 2.28 e_nll: 1.90 h_aux_loss: 0.91 e_aux_loss: 0.79 acc_0: 50.78 acc_1: 66.96 acc_2: 77.77 e_rmse: 0.31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "traininig_loop(\n",
    "    mdrnn,\n",
    "    optimizer=torch.optim.Adam(mdrnn.parameters()),\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    loss_fn=get_mdn_loss_fn(pretraining=False, score_weight=1.0),\n",
    "    eval_fn=evaluate_mdrnn,\n",
    "    num_steps=25000,\n",
    "    eval_interval=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
